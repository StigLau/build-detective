#!/usr/bin/env python3
"""
Haiku CI Analyzer - Real Haiku integration for Build Detective
Based on Komposteur's successful Haiku MCP client pattern.

This provides REAL cost-effective CI analysis using direct Haiku API calls,
not limited by Claude Code Task tool constraints.
"""

import json
import asyncio
import logging
import os
import subprocess
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

# Note: In production, would use actual Anthropic API client
# from anthropic import AsyncAnthropic

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CIAnalysisResult:
    """Results from Haiku CI analysis"""
    status: str  # SUCCESS, FAILURE, PARTIAL
    primary_error: str
    error_type: str
    confidence: int
    blocking_vs_warning: str
    suggested_action: str
    github_commands: List[str]
    estimated_cost: float
    analysis_time: float

@dataclass
class CIFailure:
    """CI failure information"""
    job_name: str
    workflow_name: str
    run_id: str
    conclusion: str
    logs: Optional[str] = None

class HaikuCIAnalyzer:
    """Real Haiku-powered CI analyzer for cost-effective pattern recognition"""
    
    def __init__(self, config_path: str = "config/haiku-config.json"):
        self.config = self._load_config(config_path)
        self.cost_tracker = CostTracker(self.config.get('cost_limits', {}))
        # Import here to avoid dependency issues if anthropic not installed
        try:
            from anthropic import AsyncAnthropic
            self.anthropic_client = AsyncAnthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
            self.has_anthropic = True
        except ImportError:
            logger.warning("Anthropic library not installed, using subprocess call to claude")
            self.anthropic_client = None
            self.has_anthropic = False
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load configuration from JSON file"""
        try:
            with open(config_path, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.warning(f"Config file {config_path} not found, using defaults")
            return self._default_config()
    
    def _default_config(self) -> Dict[str, Any]:
        """Default configuration for Haiku CI analyzer"""
        return {
            "haiku": {
                "model": "claude-3-haiku-20240307",
                "max_tokens": 800,  # Cost optimization
                "temperature": 0.1  # Consistent analysis
            },
            "cost_limits": {
                "daily_limit": 5.00,  # $5/day limit
                "operation_limit": 0.10  # $0.10 per analysis max
            },
            "patterns": {
                "yolo_ffmpeg_mcp": {
                    "uv_dependency": "pytest not available|--extra dev",
                    "docker_malformed": "=\\d+\\.\\d+\\.\\d+",
                    "python_import": "MCP module imports failed|ImportError",
                    "cache_issues": "cache.*failed|checksum.*not found"
                }
            }
        }
    
    async def analyze_actions_run(self, run_url: str) -> CIAnalysisResult:
        """
        Analyze GitHub Actions run failures using Haiku
        
        Args:
            run_url: GitHub Actions run URL (e.g., 'https://github.com/StigLau/komposteur/actions/runs/17050578304')
            
        Returns:
            CIAnalysisResult with Haiku-powered analysis
        """
        start_time = datetime.now()
        
        if not self.cost_tracker.can_proceed():
            raise CostLimitExceededException("Daily cost limit exceeded")
        
        try:
            # Parse repo and run ID from URL
            parts = run_url.replace('https://github.com/', '').split('/')
            if len(parts) < 4 or parts[2] != 'actions' or parts[3] != 'runs':
                raise ValueError(f"Invalid GitHub Actions URL: {run_url}")
            
            repo = f"{parts[0]}/{parts[1]}"
            run_id = parts[4]
            
            logger.info(f"üîç Haiku analyzing Actions run {run_id} for {repo}...")
            
            # Get run details and failures
            failures = await self._get_run_failures(repo, run_id)
            
            if not failures:
                return CIAnalysisResult(
                    status="SUCCESS",
                    primary_error="No CI failures found",
                    error_type="none",
                    confidence=10,
                    blocking_vs_warning="SUCCESS",
                    suggested_action="All checks passing",
                    github_commands=[],
                    estimated_cost=0.01,
                    analysis_time=(datetime.now() - start_time).total_seconds()
                )
            
            # Get logs for failed jobs
            enriched_failures = await self._enrich_failures_with_logs(repo, failures)
            
            # Use Haiku to analyze failure patterns
            analysis_prompt = self._create_ci_analysis_prompt(enriched_failures, repo)
            logger.info(f"üìù Haiku prompt:\n{analysis_prompt}")
            haiku_response = await self._call_haiku(analysis_prompt)
            
            # Parse Haiku response
            result = self._parse_haiku_response(haiku_response, failures)
            result.analysis_time = (datetime.now() - start_time).total_seconds()
            
            # Track cost
            self.cost_tracker.record_operation("ci_analysis", result.estimated_cost)
            
            logger.info(f"‚úÖ Haiku analysis complete: {result.confidence}/10 confidence, ${result.estimated_cost:.4f}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Haiku CI analysis failed: {e}")
            return self._fallback_analysis(failures if 'failures' in locals() else [])

    async def analyze_pr_failures(self, repo: str, pr_number: str) -> CIAnalysisResult:
        """
        Analyze PR CI failures using Haiku for cost-effective pattern recognition
        
        Args:
            repo: GitHub repository (e.g., 'StigLau/yolo-ffmpeg-mcp')
            pr_number: PR number to analyze
            
        Returns:
            CIAnalysisResult with Haiku-powered analysis
        """
        start_time = datetime.now()
        
        if not self.cost_tracker.can_proceed():
            raise CostLimitExceededException("Daily cost limit exceeded")
            
        try:
            logger.info(f"üîç Haiku analyzing PR{pr_number} failures...")
            
            # Step 1: Get PR CI status via GitHub CLI
            failures = await self._get_pr_failures(repo, pr_number)
            
            if not failures:
                return CIAnalysisResult(
                    status="SUCCESS",
                    primary_error="No CI failures found",
                    error_type="none",
                    confidence=10,
                    blocking_vs_warning="SUCCESS",
                    suggested_action="All checks passing",
                    github_commands=[],
                    estimated_cost=0.01,
                    analysis_time=(datetime.now() - start_time).total_seconds()
                )
            
            # Step 2: Get logs for failed jobs
            enriched_failures = await self._enrich_failures_with_logs(repo, failures)
            
            # Step 3: Use Haiku to analyze failure patterns
            analysis_prompt = self._create_ci_analysis_prompt(enriched_failures, repo)
            logger.info(f"üìù Haiku prompt:\n{analysis_prompt}")
            haiku_response = await self._call_haiku(analysis_prompt)
            
            # Step 4: Parse Haiku response
            result = self._parse_haiku_response(haiku_response, failures)
            result.analysis_time = (datetime.now() - start_time).total_seconds()
            
            # Track cost
            self.cost_tracker.record_operation("ci_analysis", result.estimated_cost)
            
            logger.info(f"‚úÖ Haiku analysis complete: {result.confidence}/10 confidence, ${result.estimated_cost:.4f}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Haiku CI analysis failed: {e}")
            return self._fallback_analysis(failures if 'failures' in locals() else [])
    
    async def _get_run_failures(self, repo: str, run_id: str) -> List[CIFailure]:
        """Get failures from a specific GitHub Actions run"""
        try:
            cmd = ['gh', 'run', 'view', run_id, '--repo', repo, '--json', 'jobs']
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            data = json.loads(result.stdout)
            failures = []
            
            for job in data.get('jobs', []):
                if job.get('conclusion') == 'failure':
                    failures.append(CIFailure(
                        job_name=job['name'],
                        workflow_name=data.get('name', 'Unknown'),
                        run_id=run_id,
                        conclusion='FAILURE'
                    ))
            
            return failures
            
        except subprocess.CalledProcessError as e:
            logger.error(f"GitHub CLI error getting run {run_id}: {e}")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            return []

    async def _get_pr_failures(self, repo: str, pr_number: str) -> List[CIFailure]:
        """Get PR failures using GitHub CLI"""
        try:
            cmd = ['gh', 'pr', 'view', pr_number, '--repo', repo, '--json', 'statusCheckRollup']
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            data = json.loads(result.stdout)
            failures = []
            
            for check in data.get('statusCheckRollup', []):
                if check.get('conclusion') == 'FAILURE':
                    # Extract run ID from details URL
                    run_id = None
                    if 'detailsUrl' in check and '/runs/' in check['detailsUrl']:
                        run_id = check['detailsUrl'].split('/runs/')[1].split('/')[0]
                    
                    failures.append(CIFailure(
                        job_name=check['name'],
                        workflow_name=check.get('workflowName', 'Unknown'),
                        run_id=run_id,
                        conclusion='FAILURE'
                    ))
            
            return failures
            
        except subprocess.CalledProcessError as e:
            logger.error(f"GitHub CLI error: {e}")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"JSON parsing error: {e}")
            return []
    
    async def _enrich_failures_with_logs(self, repo: str, failures: List[CIFailure]) -> List[CIFailure]:
        """Enrich failures with log snippets for Haiku analysis"""
        enriched = []
        
        for failure in failures:
            if failure.run_id:
                try:
                    # Get focused log excerpt (last 50 lines of errors)
                    cmd = ['gh', 'run', 'view', failure.run_id, '--repo', repo, '--log']
                    result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                    
                    if result.returncode == 0:
                        # Extract error patterns from logs (cost optimization)
                        log_excerpt = self._extract_error_patterns(result.stdout)
                        failure.logs = log_excerpt
                        
                except subprocess.TimeoutExpired:
                    logger.warning(f"Timeout getting logs for {failure.run_id}")
                except Exception as e:
                    logger.warning(f"Failed to get logs for {failure.run_id}: {e}")
            
            enriched.append(failure)
        
        return enriched
    
    def _extract_error_patterns(self, full_logs: str) -> str:
        """Extract key error patterns from logs for cost-effective Haiku analysis"""
        # Focus on error indicators to minimize tokens
        error_indicators = [
            "ERROR", "FAILED", "fatal:", "exit code 1",
            "not found", "ImportError", "ModuleNotFoundError",
            "=\\d+\\.\\d+\\.\\d+",  # Malformed UV files
            "pytest.*spawn", "--extra dev"
        ]
        
        lines = full_logs.split('\n')
        relevant_lines = []
        
        for line in lines:
            for indicator in error_indicators:
                if indicator.lower() in line.lower():
                    relevant_lines.append(line.strip())
                    break
            
            # Limit to 30 lines for cost control
            if len(relevant_lines) >= 30:
                break
        
        return '\n'.join(relevant_lines[-30:])  # Last 30 error lines
    
    def _create_ci_analysis_prompt(self, failures: List[CIFailure], repo: str) -> str:
        """Create optimized prompt for Haiku CI analysis"""
        failure_summary = []
        
        for failure in failures:
            summary = f"Job: {failure.job_name}\nWorkflow: {failure.workflow_name}"
            if failure.logs:
                summary += f"\nKey Errors:\n{failure.logs[:500]}"  # Limit log size
            failure_summary.append(summary)
        
        return f"""Analyze CI failures for {repo} using Build Detective patterns:

{chr(10).join(failure_summary)}

Apply YOLO-FFMPEG-MCP error patterns:
- UV dependency issues: pytest missing, --extra dev flag needed
- Docker malformed files: =X.X.X version files from UV parsing errors
- Python imports: MCP module resolution failures
- Cache problems: Docker layer or dependency cache issues

Return ONLY JSON:
{{
  "status": "FAILURE|PARTIAL|SUCCESS",
  "primary_error": "Main blocking error",
  "error_type": "dependency|docker_build|python_import|cache|workflow",
  "confidence": 8,
  "blocking_vs_warning": "BLOCKING|WARNING", 
  "suggested_action": "Specific fix command or approach",
  "github_commands": ["gh run view <id> --log"]
}}

Focus on actionable solutions. Be concise for cost efficiency."""
    
    async def _call_haiku(self, prompt: str) -> str:
        """Call Claude Haiku API directly using available Claude Code connection"""
        logger.info("ü§ñ Calling Haiku API for CI analysis...")
        
        if self.has_anthropic and self.anthropic_client:
            try:
                response = await self.anthropic_client.messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=800,
                    temperature=0.1,
                    messages=[{"role": "user", "content": prompt}]
                )
                return response.content[0].text
            except Exception as e:
                logger.error(f"Anthropic API call failed: {e}, falling back to subprocess")
        
        # Fallback: Use Claude Code's subprocess approach
        import tempfile
        import subprocess
        
        try:
            # Write prompt to temp file
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                f.write(prompt)
                temp_file = f.name
            
            # Call claude with the prompt file using proper Claude Code CLI syntax
            cmd = ['claude', '--print', prompt]
            result = subprocess.run(cmd, input=prompt, capture_output=True, text=True, timeout=60)
            
            if result.returncode == 0:
                response = result.stdout.strip()
                logger.info(f"üîç Claude response: {response[:200]}...")
                return response
            else:
                logger.error(f"Claude subprocess failed: {result.stderr}")
                raise Exception(f"Claude call failed: {result.stderr}")
                
        except Exception as e:
            logger.error(f"Claude subprocess call failed: {e}")
            # Final fallback to basic analysis
            return self._emergency_fallback_response(prompt)
        finally:
            # Clean up temp file
            try:
                import os
                os.unlink(temp_file)
            except:
                pass

    def _emergency_fallback_response(self, prompt: str) -> str:
        """Emergency fallback when all Haiku calls fail"""
        logger.warning("Using emergency fallback response")
        
        # Basic pattern matching for common issues
        if "Integration Tests with AWS" in prompt:
            return '''{
                "status": "FAILURE",
                "primary_error": "Integration tests with AWS failed - authentication or configuration issue",
                "error_type": "integration_test",
                "confidence": 6,
                "blocking_vs_warning": "BLOCKING",
                "suggested_action": "Check AWS credentials and test environment configuration",
                "github_commands": ["gh run view --log"]
            }'''
        elif "docker" in prompt.lower() or "build" in prompt.lower():
            return '''{
                "status": "FAILURE", 
                "primary_error": "Build or Docker-related failure detected",
                "error_type": "docker_build",
                "confidence": 5,
                "blocking_vs_warning": "BLOCKING",
                "suggested_action": "Review build logs and Docker configuration",
                "github_commands": ["gh run view --log"]
            }'''
        else:
            return '''{
                "status": "PARTIAL",
                "primary_error": "CI failure detected - manual analysis needed",
                "error_type": "unknown",
                "confidence": 3,
                "blocking_vs_warning": "BLOCKING",
                "suggested_action": "Manual review of CI logs required",
                "github_commands": ["gh run view --log"]
            }'''
    
    def _parse_haiku_response(self, response: str, failures: List[CIFailure]) -> CIAnalysisResult:
        """Parse Haiku JSON response into structured result"""
        try:
            data = json.loads(response)
            
            return CIAnalysisResult(
                status=data.get('status', 'PARTIAL'),
                primary_error=data.get('primary_error', 'Unknown error'),
                error_type=data.get('error_type', 'unknown'),
                confidence=data.get('confidence', 7),
                blocking_vs_warning=data.get('blocking_vs_warning', 'BLOCKING'),
                suggested_action=data.get('suggested_action', 'Manual investigation needed'),
                github_commands=data.get('github_commands', []),
                estimated_cost=0.03,  # Typical Haiku cost for this analysis
                analysis_time=0.0  # Will be set by caller
            )
            
        except json.JSONDecodeError:
            logger.warning("Failed to parse Haiku response, using fallback")
            return self._fallback_analysis(failures)
    
    def _fallback_analysis(self, failures: List[CIFailure]) -> CIAnalysisResult:
        """Fallback analysis when Haiku fails"""
        return CIAnalysisResult(
            status="PARTIAL",
            primary_error="Haiku analysis unavailable, manual review needed",
            error_type="analysis_failure", 
            confidence=3,
            blocking_vs_warning="WARNING",
            suggested_action="Use manual analysis or retry Haiku call",
            github_commands=["gh run list --status failure --limit 5"],
            estimated_cost=0.01,
            analysis_time=0.1
        )

class CostTracker:
    """Track and limit costs for Haiku operations - same as Komposteur pattern"""
    
    def __init__(self, limits: Dict[str, float]):
        self.daily_limit = limits.get('daily_limit', 5.00)
        self.operation_limit = limits.get('operation_limit', 0.10)
        self.daily_cost = 0.0
        
    def can_proceed(self, operation_cost: float = 0.03) -> bool:
        """Check if operation can proceed within cost limits"""
        return (self.daily_cost + operation_cost) <= self.daily_limit and operation_cost <= self.operation_limit
    
    def record_operation(self, operation_type: str, cost: float):
        """Record cost of completed operation"""
        self.daily_cost += cost
        logger.info(f"üí∞ {operation_type} cost: ${cost:.4f}, daily total: ${self.daily_cost:.2f}")

class CostLimitExceededException(Exception):
    """Exception raised when cost limits are exceeded"""
    pass

# CLI interface matching our ./bd pattern
async def main():
    """CLI entry point for Haiku CI analysis"""
    import sys
    
    if len(sys.argv) != 3:
        print("Usage:")
        print("  python haiku_ci_analyzer.py <repo> <pr_number>")
        print("  python haiku_ci_analyzer.py analyze <github_actions_url>")
        print("Examples:")
        print("  python haiku_ci_analyzer.py StigLau/yolo-ffmpeg-mcp 16")
        print("  python haiku_ci_analyzer.py analyze https://github.com/StigLau/komposteur/actions/runs/17050578304")
        return 1
    
    analyzer = HaikuCIAnalyzer()
    
    try:
        # Check if this is an Actions URL analysis
        if sys.argv[1] == "analyze" and "github.com" in sys.argv[2] and "/actions/runs/" in sys.argv[2]:
            run_url = sys.argv[2]
            result = await analyzer.analyze_actions_run(run_url)
            
            # Extract repo from URL for display
            parts = run_url.replace('https://github.com/', '').split('/')
            repo = f"{parts[0]}/{parts[1]}"
            run_id = parts[4]
            
            print(f"ü§ñ Haiku CI Analysis - analyze {repo}#{run_id}")
            print("=" * 60)
        else:
            # Traditional PR analysis
            repo = sys.argv[1]
            pr_number = sys.argv[2]
            result = await analyzer.analyze_pr_failures(repo, pr_number)
            
            print(f"ü§ñ Haiku CI Analysis - {repo} PR#{pr_number}")
            print("=" * 60)
        
        print(f"Status: {result.status}")
        print(f"Primary Error: {result.primary_error}")
        print(f"Type: {result.error_type}")
        print(f"Confidence: {result.confidence}/10")
        print(f"Classification: {result.blocking_vs_warning}")
        print(f"üí° Action: {result.suggested_action}")
        print(f"üí∞ Cost: ${result.estimated_cost:.4f}")
        print(f"‚è±Ô∏è Time: {result.analysis_time:.2f}s")
        
        if result.github_commands:
            print(f"üîß Commands: {', '.join(result.github_commands)}")
        
        return 0
        
    except CostLimitExceededException as e:
        print(f"üí∏ Cost limit exceeded: {e}")
        return 1
    except Exception as e:
        print(f"‚ùå Analysis failed: {e}")
        return 1

if __name__ == "__main__":
    asyncio.run(main())